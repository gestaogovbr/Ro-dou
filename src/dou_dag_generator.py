"""
Dynamic DAG generator with YAML config system to create DAG which
searches terms in the Gazzete [Diário Oficial da União-DOU] and send it
by email to the  provided `recipient_emails` list. The DAGs are
generated by YAML config files at `dag_confs` folder.

TODO:
[] - Choose the title suffix for the email using the configuration.
"""
import ast
import json
import logging
import os
import sys
import textwrap
from datetime import datetime, timedelta, date
from typing import Dict, List, Optional, Union
from functools import reduce

import pandas as pd
from airflow import DAG, Dataset
from airflow.models import Variable
from airflow.models.param import Param
from airflow.utils.task_group import TaskGroup
from airflow.hooks.base import BaseHook
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import BranchPythonOperator, PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.slack.notifications.slack import SlackNotifier
from airflow.timetables.datasets import DatasetOrTimeSchedule
from airflow.timetables.trigger import CronTriggerTimetable

try:
    from airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook
except ImportError:
    MsSqlHook = None

sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))
from utils.date import get_trigger_date, template_ano_mes_dia_trigger_local_time
from notification.notifier import Notifier
from parsers import DAGConfig, YAMLParser
from schemas import FetchTermsConfig
from searchers import BaseSearcher, DOUSearcher, QDSearcher, INLABSSearcher
from airflow.utils.email import send_email

SearchResult = Dict[str, Dict[str, Dict[str, List[dict]]]]


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================


def merge_results(*dicts: SearchResult) -> SearchResult:
    """
    Merge multiple dictionaries and sum/concatenate values of common keys,
    including nested dictionaries.
    """

    def merge_two(dict1, dict2):
        merged = {}

        # Combine keys from both dictionaries
        all_keys = set(dict1) | set(dict2)

        for key in all_keys:
            value1 = dict1.get(key)
            value2 = dict2.get(key)

            if isinstance(value1, dict) and isinstance(value2, dict):
                # If both values are dictionaries, merge them recursively
                merged[key] = merge_results(value1, value2)
            elif isinstance(value1, dict) or isinstance(value2, dict):
                # If one of the values is a dict, prefer the dict (override with dict)
                merged[key] = value1 if isinstance(value1, dict) else value2
            else:
                # Sum or concatenate the values, treating None or missing keys as 0
                merged[key] = (value1) + (value2)

        return merged

    # Pre-processing step: Filter out dictionaries where the first key's value is empty
    filtered_dicts = []

    for d in dicts:
        if d:  # Ensure the dictionary is not empty
            first_key = next(iter(d))
            first_value = d.get(first_key)
            if first_value:  # Only include if the first key's value is non-empty
                filtered_dicts.append(d)

    # If no dictionaries remain after filtering, return an empty dictionary
    if not filtered_dicts:
        return {}

    # Reduce the list of dictionaries by merging them two at a time
    return reduce(merge_two, filtered_dicts)


def result_as_html(specs: DAGConfig) -> bool:
    """Use HTML results only for emails"""
    return bool(not (specs.report.discord or specs.report.slack))


# ============================================================================
# LLM / AI INTEGRATION - LLMSummarizer CLASS
# ============================================================================


class LLMSummarizer:
    """
    Encapsulates all AI/LLM integration logic for generating summaries
    of publications. Handles interactions with multiple LLM providers
    (Gemini, OpenAI) and prompt construction.

    Responsibilities:
    - Checking if LLM is enabled and configured
    - Preparing context for AI processing
    - Building prompts for general and individual summaries
    - Calling different LLM APIs (Gemini, OpenAI)
    - Generating overall and individual publication summaries
    """

    def __init__(self):
        """Initialize the LLMSummarizer with configuration from Airflow variables"""
        self.llm_enabled = Variable.get("LLM_ENABLED", default_var="false").lower() == "true"
        self.llm_provider = Variable.get("LLM_PROVIDER", default_var="disabled").lower()
        self.llm_api_key = Variable.get("LLM_API_KEY", default_var=None)

    def is_enabled(self) -> bool:
        """Check if LLM functionality is enabled"""
        return self.llm_enabled

    def prepare_ai_context(self, search_results: list, summary: dict, report_date: str) -> dict:
        """
        Prepares structured data for AI processing from search results.

        Args:
            search_results: List of search results from different searches
            summary: Statistics summary from all searches
            report_date: The date of the report for context

        Returns:
            Dictionary with prepared AI context containing publications and stats
        """
        publications_for_ai = []

        for search in search_results:
            if not search or "result" not in search:
                continue

            result = search["result"]
            header = search.get("header", "")

            for source, sections in result.items():
                if not isinstance(sections, dict):
                    continue

                for section_name, section_data in sections.items():
                    if not isinstance(section_data, dict):
                        continue

                    for key, value in section_data.items():
                        if isinstance(value, list):
                            for pub in value:
                                if isinstance(pub, dict):
                                    publications_for_ai.append({
                                        "source": source,
                                        "section": section_name,
                                        "header": header,
                                        "title": pub.get("title", ""),
                                        "abstract": pub.get("abstract", ""),
                                        "content": pub.get("abstract", "")[:1000],
                                        "date": pub.get("date", ""),
                                        "hierarchy": pub.get("hierarchyStr", ""),
                                        "type": pub.get("arttype", ""),
                                        "href": pub.get("href", ""),
                                    })

        logging.info(f"Total publications prepared for AI: {len(publications_for_ai)}")

        ai_context = {
            "summary_stats": summary,
            "publications": publications_for_ai[:30],
            "report_date": report_date
        }

        return ai_context

    def _build_ai_prompt(self, summary_stats: dict, publications: list, report_date: str) -> str:
        """
        Constructs the prompt for overall AI summary generation.

        Args:
            summary_stats: Statistics about the publications
            publications: List of publications to analyze
            report_date: Date of the report

        Returns:
            Formatted prompt string for the LLM
        """
        total_pubs = summary_stats.get('total_publications', 0)

        # Adjusts detail level based on number of publications
        if total_pubs < 5:
            num_temas = 2
            num_publicacoes = 2
            word_limit = 300
        elif total_pubs < 20:
            num_temas = 3
            num_publicacoes = 3
            word_limit = 400
        else:
            num_temas = 5
            num_publicacoes = 5
            word_limit = 600

        max_pubs_para_analise = min(10, total_pubs // 2 + 5)

        prompt = f"""Você é um assistente especializado em análise de publicações do Diário Oficial da União (DOU). Seja simples e não fuja do texto original.

Analise as seguintes publicações do Diário Oficial de {report_date}:

ESTATÍSTICAS GERAIS:
- Total de publicações encontradas: {total_pubs}
- Fontes consultadas: {', '.join(summary_stats.get('publications_by_source', {}).keys())}
- Buscas com resultados: {summary_stats.get('searches_with_results', 0)}/{summary_stats.get('total_searches', 0)}

DISTRIBUIÇÃO POR FONTE:
"""
        for source, count in summary_stats.get('publications_by_source', {}).items():
            prompt += f"- {source}: {count} publicações\n"

        prompt += f"\n\nPUBLICAÇÕES PARA ANÁLISE (amostra de {len(publications)}):\n\n"

        for idx, pub in enumerate(publications[:max_pubs_para_analise], 1):
            prompt += f"{idx}. [{pub.get('source', 'N/A')}] {pub.get('section', 'N/A')}\n"
            prompt += f"   Título: {pub.get('title', 'Sem título')}\n"
            if pub.get('abstract'):
                prompt += f"   Resumo: {pub.get('abstract')[:300]}...\n"
            elif pub.get('content'):
                prompt += f"   Conteúdo: {pub.get('content')[:300]}...\n"
            prompt += "\n"

        prompt += f"""
Com base nas informações acima, gere um RESUMO EXECUTIVO em português que contenha:

Formato: Texto claro, conciso e em português brasileiro, com até {word_limit} palavras.
IMPORTANTE - Instruções de formatação HTML:
- Use <strong>texto</strong> para destacar palavras-chave e títulos de seções (NÃO use ** ou markdown)
- Separe os 3 parágrafos com <br><br> (quebra de linha dupla)
- Inicie cada parágrafo com 4 espaços não-quebrável (&#160;&#160;&#160;&#160;) para recuo
- Inicie cada seção com uma numeração em negrito: <strong>1. Título</strong>
- Sem listas, sem asteriscos, sem #, ##, ---, ou outros caracteres especiais

Estrutura esperada do output (copie exatamente este padrão):

&#160;&#160;&#160;&#160;<strong>1. Principais Temas e Publicações Relevantes</strong> [Parágrafo 1 aqui com <strong>destaques em negrito</strong> para palavras-chave...]<br><br>&#160;&#160;&#160;&#160;<strong>2. Tendências e Padrões Identificados</strong> [Parágrafo 2 aqui com <strong>destaques em negrito</strong>...]<br><br>&#160;&#160;&#160;&#160;<strong>3. Alertas e Pontos de Atenção</strong> [Parágrafo 3 aqui com <strong>destaques em negrito</strong>...]

Detalhes do conteúdo esperado:

Parágrafo 1: Resuma os {num_temas}-{num_temas+1} temas mais recorrentes e destaque {num_publicacoes}-{num_publicacoes+1} publicações que merecem atenção especial. Apresente de forma corrida e fluida.

Parágrafo 2: Identifique padrões interessantes nos dados (ex: aumento de publicações de um órgão específico, concentração em determinadas áreas, distribuição temporal, etc.). Apresente em forma narrativa.

Parágrafo 3: Se houver algo que demande ação imediata, publicações urgentes ou situações que se destaquem, mencione aqui. Se não houver alertas críticos, descreva brevemente os pontos mais relevantes encontrados.
"""

        return prompt

    def _call_gemini_api(self, prompt: str) -> str:
        """
        Calls Google Gemini API to generate summary.

        Args:
            prompt: The prompt to send to Gemini

        Returns:
            Generated summary text or error message
        """
        try:
            import google.generativeai as genai

            genai.configure(api_key=self.llm_api_key)
            model = genai.GenerativeModel('gemini-2.5-flash')

            response = model.generate_content(prompt)

            if response and response.text:
                return response.text
            else:
                logging.warning("Gemini returned empty response")
                return "Resumo não pôde ser gerado (resposta vazia do Gemini)"

        except ImportError:
            logging.error("google-generativeai package not installed")
            return "Erro: Pacote google-generativeai não instalado. Execute: pip install google-generativeai"
        except Exception as e:
            logging.error(f"Gemini API error: {str(e)}", exc_info=True)
            return f"Erro ao chamar Gemini API: {str(e)}"

    def _call_openai_api(self, prompt: str) -> str:
        """
        Calls OpenAI API (GPT) to generate summary.

        Args:
            prompt: The prompt to send to OpenAI

        Returns:
            Generated summary text or error message
        """
        try:
            from openai import OpenAI

            client = OpenAI(api_key=self.llm_api_key)

            response = client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "Você é um assistente especializado em análise de publicações oficiais brasileiras."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )

            if response and response.choices and len(response.choices) > 0:
                return response.choices[0].message.content
            else:
                logging.warning("OpenAI returned empty response")
                return "Resumo não pôde ser gerado (resposta vazia da OpenAI)"

        except ImportError:
            logging.error("openai package not installed")
            return "Erro: Pacote openai não instalado. Execute: pip install openai"
        except Exception as e:
            logging.error(f"OpenAI API error: {str(e)}", exc_info=True)
            return f"Erro ao chamar OpenAI API: {str(e)}"

    def call_ai_api(self, prompt: str) -> str:
        """
        Calls the configured AI API (Gemini, OpenAI, or other).

        Args:
            prompt: The prompt to send to the LLM

        Returns:
            Generated text or error message
        """
        try:
            if self.llm_provider == "disabled":
                logging.info("LLM provider is 'disabled', returning placeholder")
                return self._generate_placeholder_summary()

            if not self.llm_api_key:
                logging.error("LLM_API_KEY not configured")
                return "Erro: LLM_API_KEY não configurada nas variáveis do Airflow"

            if self.llm_provider == "gemini":
                return self._call_gemini_api(prompt)
            elif self.llm_provider == "openai":
                return self._call_openai_api(prompt)
            else:
                logging.error(f"Unsupported LLM provider: {self.llm_provider}")
                return f"Erro: Provedor LLM não suportado: {self.llm_provider}"

        except Exception as e:
            logging.error(f"Error calling AI API: {str(e)}", exc_info=True)
            return f"Erro ao chamar API de IA: {str(e)}"

    @staticmethod
    def _generate_placeholder_summary() -> str:
        """Generates a placeholder when AI is disabled"""
        return """**Resumo IA - Modo Desabilitado**

A funcionalidade de resumo inteligente está atualmente desabilitada.

Para habilitar:
1. Configure LLM_ENABLED=true
2. Configure LLM_PROVIDER (gemini ou openai)
3. Configure LLM_API_KEY com sua chave de API

Status atual das variáveis:
- LLM_ENABLED: Verificar no Airflow
- LLM_PROVIDER: Verificar no Airflow
- LLM_API_KEY: Verificar se está configurada
"""

    def generate_summary(self, summary_stats: dict, publications: list, report_date: str) -> str:
        """
        Generates overall AI summary from publications.

        Args:
            summary_stats: Statistics about publications
            publications: List of publications to summarize
            report_date: Date of the report

        Returns:
            Generated summary text or error message
        """
        try:
            if not self.is_enabled():
                logging.info("LLM disabled (LLM_ENABLED=false), skipping AI summary generation")
                return None

            if not publications:
                logging.info("No publications to summarize")
                return "Não há publicações para resumir."

            prompt = self._build_ai_prompt(summary_stats, publications, report_date)
            ai_summary = self.call_ai_api(prompt)

            logging.info(f"AI summary generated successfully ({len(ai_summary)} characters)")
            return ai_summary

        except Exception as e:
            logging.error(f"Error generating AI summary: {str(e)}", exc_info=True)
            return f"Erro ao gerar resumo IA: {str(e)}"

    def generate_individual_summaries(self, publications: list, summary_stats: dict) -> list:
        """
        Generates one-sentence summary for each publication.

        Args:
            publications: List of publications to summarize
            summary_stats: Statistics about publications

        Returns:
            List of publications with added 'ai_summary' field
        """
        try:
            if not self.is_enabled():
                logging.info("LLM disabled, skipping individual publication summaries")
                return publications

            total_pubs = summary_stats.get('total_publications', 0)

            # Determines how many publications to summarize based on total
            if total_pubs < 5:
                num_to_summarize = 3
            elif total_pubs < 20:
                num_to_summarize = 5
            else:
                num_to_summarize = 8

            publications_to_process = publications[:num_to_summarize]

            for pub in publications_to_process:
                title = pub.get('title', '')
                abstract = pub.get('abstract', '')
                content = pub.get('content', '')

                if not title or (not abstract and not content):
                    pub['ai_summary'] = ''
                    continue

                text_to_summarize = f"{title}. {abstract if abstract else content}"

                prompt = f"""Você é um assistente especializado em análise de publicações do Diário Oficial da União (DOU). Seja simples e não fuja do texto original. Resuma o seguinte texto em UMA ÚNICA FRASE clara e concisa, em português brasileiro:

Texto: {text_to_summarize[:500]}

Gere apenas a frase de resumo se não houver ementa na origem, sem explicações adicionais. A frase deve começar com a ação ou tema principal. Adicione ao início do texto 'Texto sem ementa na origem, Segue resumo gerado por IA.' """

                summary = self.call_ai_api(prompt)
                summary = summary.replace('\n', ' ').strip()

                pub['ai_summary'] = summary

                logging.info(f"Individual summary generated for: {title[:50]}...")

            return publications

        except Exception as e:
            logging.error(f"Error generating individual summaries: {str(e)}", exc_info=True)
            return publications


# ============================================================================
# MAIN DAG GENERATOR CLASS
# ============================================================================


class DouDigestDagGenerator:
    """
    YAML based Generator of DAGs that digests the DOU (gazette) daily
    publication and send email report listing all documents matching
    pré-defined keywords. It's also possible to fetch keywords from the
    database.

    Responsibilities:
    - Generating DAGs from YAML configurations
    - Executing searches across different sources (DOU, QD, INLABS)
    - Managing DAG task orchestration
    - Handling notifications on failure
    - Orchestrating AI summarization through LLMSummarizer
    """

    YAMLS_DIR = os.getenv("RO_DOU__DAG_CONF_DIR")

    if YAMLS_DIR is None:
        raise EnvironmentError("Environment variable RO_DOU__DAG_CONF_DIR not found!")

    YAMLS_DIR_LIST = [dag_confs for dag_confs in YAMLS_DIR.split(":")]
    SLACK_CONN_ID = "slack_notify_rodou_dagrun"
    DEFAULT_SCHEDULE = "0 5 * * *"

    parser = YAMLParser
    searchers: Dict[str, BaseSearcher]

    def __init__(self):
        """Initialize the DAG generator with searchers and LLM summarizer"""
        self.searchers = {
            "DOU": DOUSearcher(),
            "QD": QDSearcher(),
            "INLABS": INLABSSearcher(),
        }
        self.llm_summarizer = LLMSummarizer()
        self.on_failure_callback = self._notify_on_failure
        self.on_retry_callback = None

    # ========================================================================
    # FAILURE HANDLING
    # ========================================================================

    def _notify_on_failure(self, specs: DAGConfig, context):
        """Function called when the task fails to send a notification to admin"""
        try:
            task_instance = context.get('task_instance')
            dag_run = context.get('dag_run')
            exception = context.get('exception')

            if not task_instance or not dag_run:
                logging.error("Missing required context: task_instance or dag_run")
                return

            # Determina lista de emails para notificação
            email_list = []

            if specs.callback and specs.callback.on_failure_callback:
                # Usa emails configurados no .yaml
                email_list = specs.callback.on_failure_callback
            else:
                # Usa email admin padrão
                try:
                    email_admin = Variable.get('email_admin', default_var=None)
                    if email_admin:
                        email_list = [email_admin]
                    else:
                        logging.warning("No email_admin variable found in Airflow")
                except Exception as e:
                    logging.error(f"Error getting email_admin variable: {str(e)}")

            if not email_list:
                logging.warning("No email configured for failure notification (neither callback nor email_admin variable)")

            # Envia email se houver destinatários
            if email_list:
                try:
                    execution_date_str = dag_run.execution_date.strftime("%d/%m/%Y %H:%M") if dag_run.execution_date else "N/A"

                    send_email(
                        to=email_list,
                        subject=f'Falha na DAG {dag_run.dag_id}',
                        html_content=f'''
                        <h3>Falha detectada</h3>
                        <p><strong>DAG:</strong> {dag_run.dag_id}</p>
                        <p><strong>Task:</strong> {task_instance.task_id}</p>
                        <p><strong>Estado:</strong> {task_instance.state}</p>
                        <p><strong>Data de execução:</strong> {execution_date_str}</p>
                        <p><strong>Exceção:</strong> {exception}</p>
                        <p><strong>Log:</strong> <a href="{task_instance.log_url}" target="_blank">Ver log completo</a></p>
                        '''
                    )
                    logging.info(f"SUCCESS: Failure notification email sent to: {email_list}")
                except Exception as e:
                    logging.error(f"ERROR: Failed to send failure notification email: {str(e)}", exc_info=True)

            # Tenta enviar notificação via Slack (se disponível)
            try:
                conn = BaseHook.get_connection(self.SLACK_CONN_ID)
                description = json.loads(conn.description)
                slack_notifier = SlackNotifier(
                    slack_conn_id=self.SLACK_CONN_ID,
                    text=(
                        ":bomb: *Falha na DAG*"
                        f"\n*DAG:* `{dag_run.dag_id}`"
                        f"\n*Task:* `{task_instance.task_id}`"
                        f"\n*State:* `{task_instance.state}`"
                        f"\n*Data de execução:* {dag_run.execution_date.strftime('%d/%m/%Y %H:%M') if dag_run.execution_date else 'N/A'}"
                        f"\n*Exception:* {exception}"
                        f"\n*Log:* <{task_instance.log_url}|Ver log completo>"
                    ),
                    channel=description["channel"],
                )
                slack_notifier.notify(context)
            except Exception as e:
                logging.error(f"Slack notification not sent: {str(e)}")

        except Exception as e:
            logging.error(f"Error in _notify_on_failure: {str(e)}", exc_info=True)

    # ========================================================================
    # AI/LLM ORCHESTRATION METHODS
    # ========================================================================

    def prepare_ai_context(self, num_searches: int, **context) -> dict:
        """
        Prepares AI context from search results.
        Delegates to LLMSummarizer for actual processing.
        """
        search_results = self.get_xcom_pull_tasks(num_searches=num_searches, **context)
        summary = context["ti"].xcom_pull(task_ids="summarize_publications")
        report_date = get_trigger_date(context, local_time=True)

        ai_context = self.llm_summarizer.prepare_ai_context(search_results, summary, report_date)
        return ai_context

    def generate_ai_summary(self, num_searches: int, **context) -> str:
        """
        Generates AI summary and individual publication summaries.
        Delegates to LLMSummarizer for actual AI processing.
        """
        try:
            ai_context = context["ti"].xcom_pull(task_ids="prepare_ai_context")

            if not ai_context:
                logging.warning("AI context not found, returning empty summary")
                return "Resumo IA não disponível (contexto vazio)."

            summary_stats = ai_context.get('summary_stats', {})
            publications = ai_context.get('publications', [])
            report_date = ai_context.get('report_date', 'data desconhecida')

            # Gera o resumo geral usando o LLMSummarizer
            ai_summary = self.llm_summarizer.generate_summary(summary_stats, publications, report_date)

            # Gera resumos individuais para cada publicação
            publications_with_summaries = self.llm_summarizer.generate_individual_summaries(publications, summary_stats)

            # Armazena as publicações com resumos em XCom para uso posterior
            context["ti"].xcom_push(key="publications_with_summaries", value=publications_with_summaries)

            logging.info(f"AI summary generated successfully ({len(ai_summary) if ai_summary else 0} characters)")

            # Retorna apenas o resumo geral
            return ai_summary

        except Exception as e:
            logging.error(f"Error generating AI summary: {str(e)}", exc_info=True)
            return f"Erro ao gerar resumo IA: {str(e)}"

    # ========================================================================
    # DOCUMENTATION
    # ========================================================================

    @staticmethod
    def prepare_doc_md(specs: DAGConfig, config_file: str) -> str:
        """Prepares the markdown documentation for a dag.

        Args:
            specs (DAGConfig): A DAG configuration object.
            config_file (str): The name of a DAG config file.

        Returns:
            str: The DAG documentation in markdown format.
        """
        config = specs.model_dump()
        # options that won't show in the "DAG Docs"
        del config["description"]
        del config["doc_md"]
        doc_md = specs.doc_md + textwrap.dedent(
            f"""

            **Configuração da dag definida no arquivo `{config_file}`**:


            """
        )
        for key, value in config.items():
            doc_md += f"\n**{key.replace('_', ' ').capitalize()}**\n"

            if isinstance(value, (list, set)):
                doc_md += "\n" + "\n".join(f"- {str(item)}" for item in value) + "\n"
            else:
                doc_md += "\n" + f"- {str(value)}\n" + "\n"

        return doc_md

    # ========================================================================
    # SCHEDULING UTILITIES
    # ========================================================================

    @staticmethod
    def _hash_dag_id(dag_id: str, size: int) -> int:
        """Hashes the `dag_id` into a integer between 0 and `size`"""
        buffer = 0
        for _char in dag_id:
            buffer += ord(_char)
        try:
            _hash = buffer % size
        except ZeroDivisionError:
            raise ValueError("`size` deve ser maior que 0.")
        return _hash

    def _get_safe_schedule(self, specs: DAGConfig, default_schedule: str) -> str:
        """Return a new value of `schedule`, randomazing the
        execution minute accorgind to `dag_id`, if the dag use the
        schedule standard. Apply a function of hash to string
        dag_id to return a value between 0 and 60, wich defines the execution minute.
        """

        schedule = default_schedule
        id_based_minute = self._hash_dag_id(specs.id, 60)
        schedule_without_min = " ".join(schedule.split(" ")[1:])
        schedule = f"{id_based_minute} {schedule_without_min}"

        return schedule

    def _update_schedule_with_dataset(
        self, dataset: str, schedule: str, is_default_schedule: bool
    ) -> Union[Dataset, DatasetOrTimeSchedule]:
        """If a dataset is provide, the schedule will be update
        to make execution conditional on the Dataset or
        DatasetOrTimeSchedule
        (if the value of schedule is specified in the YAML information).
        """
        if not is_default_schedule:
            return DatasetOrTimeSchedule(
                timetable=CronTriggerTimetable(
                    schedule, timezone=os.getenv("AIRFLOW__CORE__DEFAULT_TIMEZONE")
                ),
                datasets=[Dataset(dataset)],
            )
        return [Dataset(dataset)]

    def _update_schedule(
        self, specs: DAGConfig
    ) -> Union[str, Union[Dataset, DatasetOrTimeSchedule]]:
        """The DAG will update the value of schedule to
        the default value or to a Dataset, if that option is specified.
        """
        schedule = specs.schedule

        if schedule is None:
            schedule = self._get_safe_schedule(
                specs=specs, default_schedule=self.DEFAULT_SCHEDULE
            )
            is_default_schedule = True
        else:
            is_default_schedule = False

        if specs.dataset is not None:
            schedule = self._update_schedule_with_dataset(
                dataset=specs.dataset,
                schedule=schedule,
                is_default_schedule=is_default_schedule,
            )

        return schedule

    # ========================================================================
    # DAG GENERATION AND SEARCH ORCHESTRATION
    # ========================================================================

    def generate_dags(self):
        """Iterates over the YAML files and creates all dags"""

        files_list = []

        for directory in self.YAMLS_DIR_LIST:
            for dirpath, _, filenames in os.walk(directory):
                for filename in filenames:
                    if any(ext in filename for ext in [".yaml", ".yml"]):
                        files_list.extend([os.path.join(dirpath, filename)])

        for filepath in files_list:
            dag_specs = self.parser(filepath).parse()
            dag_id = dag_specs.id
            globals()[dag_id] = self.create_dag(dag_specs, filepath)

    def _parse_term_list(self, term_list):
        """Converte term_list para lista, tratando aspas duplas extras."""
        if not isinstance(term_list, str):
            return term_list

        # Remove espaços
        term_list = term_list.strip()

        # Tenta converter até 2 vezes (para casos de serialização dupla)
        for _ in range(2):
            if term_list.startswith(('[', '"[', "'[")):
                try:
                    term_list = ast.literal_eval(term_list)
                    if isinstance(term_list, list):
                        return term_list
                except (ValueError, SyntaxError):
                    break

        return term_list

    def perform_searches(
        self,
        header,
        sources,
        territory_id,
        term_list,
        dou_sections: List[str],
        search_date,
        field,
        is_exact_search: Optional[bool],
        ignore_signature_match: Optional[bool],
        force_rematch: Optional[bool],
        full_text: Optional[bool],
        text_length: Optional[int],
        use_summary: Optional[bool],
        result_as_email: Optional[bool],
        department: List[str],
        department_ignore: List[str],
        pubtype: List[str],
        excerpt_size: Optional[int],
        number_of_excerpts: Optional[int],
        **context,
    ) -> dict:
        """Performs the search in each source and merge the results"""
        if "DOU" in sources:
            dou_result = self.searchers["DOU"].exec_search(
                term_list=term_list,
                dou_sections=dou_sections,
                search_date=search_date,
                field=field,
                is_exact_search=is_exact_search,
                ignore_signature_match=ignore_signature_match,
                force_rematch=force_rematch,
                department=department,
                department_ignore=department_ignore,
                pubtype=pubtype,
                reference_date=get_trigger_date(context, local_time=True),
            )
        elif "INLABS" in sources:
            terms = self._parse_term_list(term_list)
            inlabs_result = self.searchers["INLABS"].exec_search(
                terms=terms,
                dou_sections=dou_sections,
                search_date=search_date,
                department=department,
                department_ignore=department_ignore,
                ignore_signature_match=ignore_signature_match,
                full_text=full_text,
                text_length=text_length,
                use_summary=use_summary,
                pubtype=pubtype,
                reference_date=get_trigger_date(context, local_time=True),
            )

        if "QD" in sources:
            qd_result = self.searchers["QD"].exec_search(
                territory_id=territory_id,
                term_list=term_list,
                is_exact_search=is_exact_search,
                reference_date=get_trigger_date(context, local_time=True),
                excerpt_size=excerpt_size,
                number_of_excerpts=number_of_excerpts,
                result_as_email=result_as_email,
            )

        if "DOU" in sources and "QD" in sources:
            result = merge_results(qd_result, dou_result)
        elif "INLABS" in sources and "QD" in sources:
            result = merge_results(qd_result, inlabs_result)
        elif "DOU" in sources:
            result = dou_result
        elif "INLABS" in sources:
            result = inlabs_result
        else:
            result = qd_result

        # Add more specs info
        search_dict = {}
        search_dict["result"] = result
        search_dict["header"] = header
        search_dict["department"] = department
        search_dict["department_ignore"] = department_ignore
        search_dict["pubtype"] = pubtype

        return search_dict

    def get_xcom_pull_tasks(self, num_searches, **context):
        """Retrieve XCom values from multiple tasks and append them to a new list.
        Function required for Airflow version 2.10.0 or later
        (https://github.com/apache/airflow/issues/41983).
        """
        search_results = []
        for counter in range(1, num_searches + 1):
            search_results.append(
                context["ti"].xcom_pull(task_ids=f"exec_searchs.exec_search_{counter}")
            )

        return search_results

    def has_matches(self, num_searches: int, skip_null: bool, **context) -> str:
        """Check if search has matches and return to skip notification or not"""

        if skip_null:
            search_results = self.get_xcom_pull_tasks(
                num_searches=num_searches, **context
            )

            skip_notification = True

            for search in search_results:
                if search and search.get("result"):
                    items = ["contains" for k, v in search["result"].items() if v]
                    if items:
                        skip_notification = False

            return "skip_notification" if skip_notification else "summarize_publications"
        else:
            return "summarize_publications"

    def summarize_publications(self, num_searches: int, **context) -> dict:
        """
        Summarizes publications from all searches before sending notification.
        Returns statistics and prepares data for potential AI enhancement.
        """
        search_results = self.get_xcom_pull_tasks(num_searches=num_searches, **context)

        summary = {
            "total_searches": num_searches,
            "searches_with_results": 0,
            "total_publications": 0,
            "publications_by_source": {},
            "publications_by_section": {},
            "publications_by_header": {},
            "has_content": False
        }
        logging.info(f"Searching result {search_results} searches")

        for idx, search in enumerate(search_results, 1):
            if not search or not search.get("result"):
                continue

            result = search["result"]
            header = search.get("header", f"Search {idx}")

            logging.info(f"DEBUG: Processing search {idx} with header '{header}'")
            logging.info(f"DEBUG: Result keys: {list(result.keys())}")

            publication_count = 0

            for source, categories in result.items():
                logging.info(f"DEBUG: Source: {source}")

                if source not in summary["publications_by_source"]:
                    summary["publications_by_source"][source] = 0

                for category, content in categories.items():
                    logging.info(f"DEBUG: Category: {category}, Type of content: {type(content)}")
                    if isinstance(content, dict) and 'single_department' in content:
                        pubs = content['single_department']
                        if isinstance(pubs, list):
                            pub_count = len(pubs)
                            summary["publications_by_source"][source] += pub_count
                            summary["total_publications"] += pub_count
                            publication_count += pub_count

                            # Registrar por seção
                            section_key = f"{source}_{category}"
                            if section_key not in summary["publications_by_section"]:
                                summary["publications_by_section"][section_key] = 0
                            summary["publications_by_section"][section_key] += pub_count

                            logging.info(f"DEBUG: Found {pub_count} publications in {category}")
                        else:
                            logging.warning(f"DEBUG: 'single_department' is not a list in {category}")
                    else:
                        logging.warning(f"DEBUG: Unexpected content structure in {category}: {type(content)}")

            if publication_count > 0:
                summary["searches_with_results"] += 1
                summary["has_content"] = True

                if header not in summary["publications_by_header"]:
                    summary["publications_by_header"][header] = 0
                summary["publications_by_header"][header] += publication_count

                logging.info(f"DEBUG: Search {idx} has {publication_count} publications")
            else:
                logging.info(f"DEBUG: Search {idx} has 0 publications")

        logging.info(
            f"Summary generated: {summary['total_publications']} publications "
            f"from {summary['searches_with_results']}/{num_searches} searches"
        )
        logging.info(f"Publications by source: {summary['publications_by_source']}")
        logging.info(f"Publications by header: {summary['publications_by_header']}")

        return summary

    # ========================================================================
    # NOTIFICATIONS
    # ========================================================================

    def send_notification(
        self, num_searches: int, specs: DAGConfig, report_date: str, **context
    ) -> str:
        """Send user notification using class Notifier"""
        search_report = self.get_xcom_pull_tasks(num_searches=num_searches, **context)

        # Recupera o summary se disponível
        summary = context["ti"].xcom_pull(task_ids="summarize_publications")
        # Recupera o resumo da IA se disponível
        ai_summary = context["ti"].xcom_pull(task_ids="generate_ai_summary")
        # Recupera as publicações com resumos individuais
        publications_with_summaries = context["ti"].xcom_pull(
            task_ids="generate_ai_summary", key="publications_with_summaries"
        )

        notifier = Notifier(specs)

        logging.info(f"Summary: {summary}")
        logging.info(f"IA Summary: {ai_summary}")
        logging.info(f"Publications with summaries count: {len(publications_with_summaries) if publications_with_summaries else 0}")

        # Passa o summary, ai_summary e publicações com resumos para o notifier
        notifier.send_notification(
            search_report=search_report,
            report_date=report_date,
            ai_summary=ai_summary,
            publications_with_summaries=publications_with_summaries
        )

    # ========================================================================
    # DATABASE AND CONFIGURATION UTILITIES
    # ========================================================================

    def select_terms_from_airflow_variable(self, variable: str):
        """
        Retrieves and processes a list of terms from an Apache Airflow variable.

        This function searches for a specific Airflow variable and converts it into a list
        of terms, supporting both JSON and line-delimited text formats.

        Arguments:
        variable (str): Name of the Airflow variable to be retrieved.

        Returns:
        list: List of terms extracted from the Airflow variable.

        Raises:
        KeyError: When a specified variable was not found in Airflow.
        """

        term_list = []
        var_name = variable

        try:
            var_value = Variable.get(var_name)
            # Se já é uma lista, retorna direto
            if isinstance(var_value, list):
                return var_value

            if isinstance(var_value, str):
                if var_value.strip().startswith('['):
                    return ast.literal_eval(var_value)
                else:
                    # Trata como texto separado por linhas
                    return var_value.splitlines()
            return term_list

        except (KeyError):
            raise KeyError(
                f"Airflow variable {var_name} not found."
            )

    def select_terms_from_db(self, sql: str, conn_id: str):
        """Queries the `sql` and return the list of terms that will be
        used later in the DOU search. The first column of the select
        must contain the terms to be searched. The second column, which
        is optional, is a classifier that will be used to group and sort
        the email report and the generated CSV.
        """
        conn_type = BaseHook.get_connection(conn_id).conn_type
        if conn_type == "mssql":
            if MsSqlHook is None:
                raise RuntimeError("MsSqlHook indisponível: instale 'apache-airflow-providers-microsoft-mssql' para usar recursos MSSQL.")
            db_hook = MsSqlHook(conn_id)
        elif conn_type in ("postgresql", "postgres"):
            db_hook = PostgresHook(conn_id)
        else:
            raise Exception("Tipo de banco de dados não suportado: ", conn_type)

        terms_df = db_hook.get_pandas_df(sql)
        # Remove unnecessary spaces and change null for ''
        terms_df = terms_df.applymap(lambda x: str.strip(x) if pd.notnull(x) else "")

        return terms_df.to_json(orient="columns")

    # ========================================================================
    # DAG CREATION
    # ========================================================================

    def create_dag(self, specs: DAGConfig, config_file: str) -> DAG:
        """Creates the DAG object and tasks

        Depending on configuration it adds an extra prior task to query
        the term_list from a database
        """
        # Prepare the markdown documentation
        doc_md = self.prepare_doc_md(specs, config_file) if specs.doc_md else None
        # DAG parameters
        default_args = {
            "owner": ",".join(specs.owner),
            "start_date": datetime(2021, 10, 18),
            "depends_on_past": False,
            "retries": 10,
            "retry_delay": timedelta(minutes=2),
            "on_retry_callback": self.on_retry_callback,
            "on_failure_callback": lambda context: self._notify_on_failure(specs, context),
        }

        schedule = self._update_schedule(specs)
        dag = DAG(
            specs.id,
            default_args=default_args,
            schedule=schedule,
            description=specs.description,
            doc_md=doc_md,
            catchup=False,
            params={
                "trigger_date": Param(
                    default=date.today().isoformat(), type="string", format="date"
                )
            },
            tags=list(specs.tags),
        )

        with dag:

            with TaskGroup(group_id="exec_searchs") as tg_exec_searchs:

                searches = specs.search

                for counter, subsearch in enumerate(searches, 1):
                    # Verify if the terms were fetched from the database
                    terms_come_from_db: bool = isinstance(
                        subsearch.terms, FetchTermsConfig
                    ) and getattr(subsearch.terms, "from_db_select", None)

                    terms_come_from_airflow_variable: bool = isinstance(
                        subsearch.terms, FetchTermsConfig
                    ) and getattr(subsearch.terms, "from_airflow_variable", None)

                    # determine the terms list
                    term_list = []
                    # is it a directly defined list of terms or is it a
                    # configuration for fetching terms from a data source?
                    if subsearch.terms is None:
                        # No specific terms - will search all publications with filters
                        term_list = None
                    elif isinstance(subsearch.terms, list):
                        term_list = subsearch.terms
                    elif terms_come_from_airflow_variable:
                        select_terms_from_airflow_variable_task = PythonOperator(
                            task_id=f"select_terms_from_airflow_variable_{counter}",
                            python_callable=self.select_terms_from_airflow_variable,
                            op_kwargs={
                                "variable": subsearch.terms.from_airflow_variable
                            }
                        )
                        term_list = (
                            "{{ ti.xcom_pull(task_ids='exec_searchs.select_terms_from_airflow_variable_"
                            + str(counter)
                            + "') }}"
                        )

                    elif terms_come_from_db:
                        select_terms_from_db_task = PythonOperator(
                            task_id=f"select_terms_from_db_{counter}",
                            python_callable=self.select_terms_from_db,
                            op_kwargs={
                                "sql": subsearch.terms.from_db_select.sql,
                                "conn_id": subsearch.terms.from_db_select.conn_id,
                            },
                        )
                        term_list = (
                            "{{ ti.xcom_pull(task_ids='exec_searchs.select_terms_from_db_"
                            + str(counter)
                            + "') }}"
                        )

                    exec_search_task = PythonOperator(
                        task_id=f"exec_search_{counter}",
                        python_callable=self.perform_searches,
                        op_kwargs={
                            "header": subsearch.header,
                            "sources": subsearch.sources,
                            "territory_id": subsearch.territory_id,
                            "term_list": term_list,
                            "dou_sections": subsearch.dou_sections,
                            "search_date": subsearch.date,
                            "field": subsearch.field,
                            "is_exact_search": subsearch.is_exact_search,
                            "ignore_signature_match": subsearch.ignore_signature_match,
                            "force_rematch": subsearch.force_rematch,
                            "full_text": subsearch.full_text,
                            "text_length": subsearch.text_length,
                            "use_summary": subsearch.use_summary,
                            "department": subsearch.department,
                            "department_ignore": subsearch.department_ignore,
                            "pubtype": subsearch.pubtype,
                            "excerpt_size": subsearch.excerpt_size,
                            "number_of_excerpts": subsearch.number_of_excerpts,
                            "result_as_email": result_as_html(specs),
                        },
                    )

                    if terms_come_from_airflow_variable:
                        select_terms_from_airflow_variable_task >> exec_search_task

                    if terms_come_from_db:
                        select_terms_from_db_task >> exec_search_task

            # ====================================================================
            # TASK DEFINITIONS - ORGANIZED BY FLOW
            # ====================================================================

            # Decision task: Check if there are search matches
            has_matches_task = BranchPythonOperator(
                task_id="has_matches",
                python_callable=self.has_matches,
                op_kwargs={
                    "num_searches": len(searches),
                    "skip_null": specs.report.skip_null,
                },
            )

            # Alternative task: Skip notification if no matches
            skip_notification_task = EmptyOperator(task_id="skip_notification")

            # Task: Summarize publications statistics
            summarize_publications_task = PythonOperator(
                task_id="summarize_publications",
                python_callable=self.summarize_publications,
                op_kwargs={
                    "num_searches": len(searches),
                },
            )

            # Task: Prepare context for AI processing
            prepare_ai_context_task = PythonOperator(
                task_id="prepare_ai_context",
                python_callable=self.prepare_ai_context,
                op_kwargs={"num_searches": len(searches)},
            )

            # Task: Generate AI summary and individual publication summaries
            generate_ai_summary_task = PythonOperator(
                task_id="generate_ai_summary",
                python_callable=self.generate_ai_summary,
                op_kwargs={"num_searches": len(searches)},
            )

            # Task: Send notification with all results
            send_notification_task = PythonOperator(
                task_id="send_notification",
                python_callable=self.send_notification,
                op_kwargs={
                    "num_searches": len(searches),
                    "specs": specs,
                    "report_date": template_ano_mes_dia_trigger_local_time,
                },
            )

            # ====================================================================
            # TASK FLOW ORCHESTRATION
            # ====================================================================

            # Main flow: execute searches → check for matches
            tg_exec_searchs >> has_matches_task

            # Branch 1: No matches found → skip notification
            has_matches_task >> skip_notification_task

            # Branch 2: Matches found → process and notify
            has_matches_task >> summarize_publications_task
            summarize_publications_task >> prepare_ai_context_task
            prepare_ai_context_task >> generate_ai_summary_task
            generate_ai_summary_task >> send_notification_task

        return dag


# ============================================================================
# DAG GENERATION - RUN AT MODULE LOAD TIME
# ============================================================================

# Run dag generation
DouDigestDagGenerator().generate_dags()
