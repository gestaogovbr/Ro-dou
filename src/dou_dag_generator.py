"""
Dynamic DAG generator with YAML config system to create DAG which
searches terms in the Gazzete [Diário Oficial da União-DOU] and send it
by email to the  provided `recipient_emails` list. The DAGs are
generated by YAML config files at `dag_confs` folder.

TODO:
[] - Definir sufixo do título do email a partir de configuração
"""

import logging
import os
import sys
import textwrap
from datetime import datetime, timedelta, date
from typing import Dict, List, Optional, Union
from functools import reduce
import json

import pandas as pd
from airflow import DAG, Dataset
from airflow.models.param import Param
from airflow.utils.task_group import TaskGroup
from airflow.hooks.base import BaseHook
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import BranchPythonOperator, PythonOperator
from airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.slack.notifications.slack import SlackNotifier
from airflow.timetables.datasets import DatasetOrTimeSchedule
from airflow.timetables.trigger import CronTriggerTimetable

sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))
from utils.date import get_trigger_date, template_ano_mes_dia_trigger_local_time
from notification.notifier import Notifier
from parsers import DAGConfig, YAMLParser
from schemas import FetchTermsConfig
from searchers import BaseSearcher, DOUSearcher, QDSearcher, INLABSSearcher


SearchResult = Dict[str, Dict[str, Dict[str, List[dict]]]]


def merge_results(*dicts: SearchResult) -> SearchResult:
    """
    Merge multiple dictionaries and sum/concatenate values of common keys,
    including nested dictionaries.
    """

    def merge_two(dict1, dict2):
        merged = {}

        # Combine keys from both dictionaries
        all_keys = set(dict1) | set(dict2)

        for key in all_keys:
            value1 = dict1.get(key)

            value2 = dict2.get(key)

            if isinstance(value1, dict) and isinstance(value2, dict):
                # If both values are dictionaries, merge them recursively
                merged[key] = merge_results(value1, value2)
            elif isinstance(value1, dict) or isinstance(value2, dict):
                # If one of the values is a dict, prefer the dict (override with dict)
                merged[key] = value1 if isinstance(value1, dict) else value2
            else:
                # Sum or concatenate the values, treating None or missing keys as 0
                merged[key] = (value1) + (value2)

        return merged

    # Pre-processing step: Filter out dictionaries where the first key's value is empty
    filtered_dicts = []

    for d in dicts:
        if d:  # Ensure the dictionary is not empty
            first_key = next(iter(d))
            first_value = d.get(first_key)
            if first_value:  # Only include if the first key's value is non-empty
                filtered_dicts.append(d)

    # If no dictionaries remain after filtering, return an empty dictionary
    if not filtered_dicts:
        return {}

    # Reduce the list of dictionaries by merging them two at a time
    return reduce(merge_two, filtered_dicts)


def result_as_html(specs: DAGConfig) -> bool:
    """Só utiliza resultado HTML apenas para email"""
    return bool(not (specs.report.discord or specs.report.slack))


class DouDigestDagGenerator:
    """
    YAML based Generator of DAGs that digests the DOU (gazette) daily
    publication and send email report listing all documents matching
    pré-defined keywords. It's also possible to fetch keywords from the
    database.
    """

    YAMLS_DIR = os.getenv("RO_DOU__DAG_CONF_DIR")

    if YAMLS_DIR is None:
        raise EnvironmentError("Environment variable RO_DOU__DAG_CONF_DIR not found!")

    YAMLS_DIR_LIST = [dag_confs for dag_confs in YAMLS_DIR.split(":")]
    SLACK_CONN_ID = "slack_notify_rodou_dagrun"
    DEFAULT_SCHEDULE = "0 5 * * *"

    parser = YAMLParser
    searchers: Dict[str, BaseSearcher]

    def __init__(self):
        self.searchers = {
            "DOU": DOUSearcher(),
            "QD": QDSearcher(),
            "INLABS": INLABSSearcher(),
        }
        try:
            conn = BaseHook.get_connection(self.SLACK_CONN_ID)
            description = json.loads(conn.description)
            slack_notifier = SlackNotifier(
                slack_conn_id=self.SLACK_CONN_ID,
                text=(
                    ":bomb:"
                    "\n`DAG`  {{ ti.dag_id }}"
                    "\n`State`  {{ ti.state }}"
                    "\n`Task`  {{ ti.task_id }}"
                    "\n`Execution`  {{ ti.execution_date }}"
                    "\n`Log`  {{ ti.log_url }}"
                ),
                channel=description["channel"],
            )
        except Exception as e:
            logging.info("Connection to DAG run notifier not configured: %s", str(e))
            slack_notifier = None

        self.on_failure_callback = slack_notifier
        self.on_retry_callback = None

    @staticmethod
    def prepare_doc_md(specs: DAGConfig, config_file: str) -> str:
        """Prepares the markdown documentation for a dag.

        Args:
            specs (DAGConfig): A DAG configuration object.
            config_file (str): The name of a DAG config file.

        Returns:
            str: The DAG documentation in markdown format.
        """
        config = specs.model_dump()
        # options that won't show in the "DAG Docs"
        del config["description"]
        del config["doc_md"]
        doc_md = specs.doc_md + textwrap.dedent(
            f"""

            **Configuração da dag definida no arquivo `{config_file}`**:


            """
        )
        for key, value in config.items():
            doc_md += f"\n**{key.replace('_', ' ').capitalize()}**\n"

            if isinstance(value, (list, set)):
                doc_md += "\n" + "\n".join(f"- {str(item)}" for item in value) + "\n"
            else:
                doc_md += "\n" + f"- {str(value)}\n" + "\n"
                
        return doc_md

    @staticmethod
    def _hash_dag_id(dag_id: str, size: int) -> int:
        """Hashes the `dag_id` into a integer between 0 and `size`"""
        buffer = 0
        for _char in dag_id:
            buffer += ord(_char)
        try:
            _hash = buffer % size
        except ZeroDivisionError:
            raise ValueError("`size` deve ser maior que 0.")
        return _hash

    def _get_safe_schedule(self, specs: DAGConfig, default_schedule: str) -> str:
        """Retorna um novo valor de `schedule` randomizando o
        minuto de execução baseado no `dag_id`, caso a dag utilize o
        schedule padrão. Aplica uma função de hash na string
        dag_id que retorna valor entre 0 e 60 que define o minuto de
        execução.
        """

        schedule = default_schedule
        id_based_minute = self._hash_dag_id(specs.id, 60)
        schedule_without_min = " ".join(schedule.split(" ")[1:])
        schedule = f"{id_based_minute} {schedule_without_min}"

        return schedule

    def _update_schedule_with_dataset(
        self, dataset: str, schedule: str, is_default_schedule: bool
    ) -> Union[Dataset, DatasetOrTimeSchedule]:
        """Caso informado um dataset o schedule é alterado
        para ser condicionado a execução por Dataset ou
        DatasetOrTimeSchedule
        (caso o valor de schedule esteja informado no YAML).
        """
        if not is_default_schedule:
            return DatasetOrTimeSchedule(
                timetable=CronTriggerTimetable(
                    schedule, timezone=os.getenv("AIRFLOW__CORE__DEFAULT_TIMEZONE")
                ),
                datasets=[Dataset(dataset)],
            )
        return [Dataset(dataset)]

    def _update_schedule(
        self, specs: DAGConfig
    ) -> Union[str, Union[Dataset, DatasetOrTimeSchedule]]:
        """Atualiza o valor do schedule para o
        valor default ou para Dataset, se for o caso.
        """
        schedule = specs.schedule

        if schedule is None:
            schedule = self._get_safe_schedule(
                specs=specs, default_schedule=self.DEFAULT_SCHEDULE
            )
            is_default_schedule = True
        else:
            is_default_schedule = False

        if specs.dataset is not None:
            schedule = self._update_schedule_with_dataset(
                dataset=specs.dataset,
                schedule=schedule,
                is_default_schedule=is_default_schedule,
            )

        return schedule

    def generate_dags(self):
        """Iterates over the YAML files and creates all dags"""

        files_list = []

        for directory in self.YAMLS_DIR_LIST:
            for dirpath, _, filenames in os.walk(directory):
                for filename in filenames:
                    if any(ext in filename for ext in [".yaml", ".yml"]):
                        files_list.extend([os.path.join(dirpath, filename)])

        for filepath in files_list:
            dag_specs = self.parser(filepath).parse()
            dag_id = dag_specs.id
            globals()[dag_id] = self.create_dag(dag_specs, filepath)

    def perform_searches(
        self,
        header,
        sources,
        territory_id,
        term_list,
        dou_sections: List[str],
        search_date,
        field,
        is_exact_search: Optional[bool],
        ignore_signature_match: Optional[bool],
        force_rematch: Optional[bool],
        full_text: Optional[bool],
        use_summary: Optional[bool],
        result_as_email: Optional[bool],
        department: List[str],
        pubtype: List[str],
        **context,
    ) -> dict:
        """Performs the search in each source and merge the results"""
        logging.info("Searching for: %s", term_list)
        logging.info("Trigger date: %s", get_trigger_date(context, local_time=True))

        if "DOU" in sources:
            dou_result = self.searchers["DOU"].exec_search(
                term_list=term_list,
                dou_sections=dou_sections,
                search_date=search_date,
                field=field,
                is_exact_search=is_exact_search,
                ignore_signature_match=ignore_signature_match,
                force_rematch=force_rematch,
                department=department,
                pubtype=pubtype,
                reference_date=get_trigger_date(context, local_time=True),
            )
        elif "INLABS" in sources:
            inlabs_result = self.searchers["INLABS"].exec_search(
                terms=term_list,
                dou_sections=dou_sections,
                search_date=search_date,
                department=department,
                ignore_signature_match=ignore_signature_match,
                full_text=full_text,
                use_summary=use_summary,
                pubtype=pubtype,
                reference_date=get_trigger_date(context, local_time=True),
            )

        if "QD" in sources:
            qd_result = self.searchers["QD"].exec_search(
                territory_id=territory_id,
                term_list=term_list,
                dou_sections=dou_sections,
                search_date=search_date,
                field=field,
                is_exact_search=is_exact_search,
                ignore_signature_match=ignore_signature_match,
                force_rematch=force_rematch,
                reference_date=get_trigger_date(context, local_time=True),
                result_as_email=result_as_email,
            )

        if "DOU" in sources and "QD" in sources:
            result = merge_results(qd_result, dou_result)
        elif "INLABS" in sources and "QD" in sources:
            result = merge_results(qd_result, inlabs_result)
        elif "DOU" in sources:
            result = dou_result
        elif "INLABS" in sources:
            result = inlabs_result
        else:
            result = qd_result

        # Add more specs info
        search_dict = {}
        search_dict["result"] = result
        search_dict["header"] = header
        search_dict["department"] = department
        search_dict["pubtype"] = pubtype

        return search_dict

    def get_xcom_pull_tasks(self, num_searches, **context):
        """Retrieve XCom values from multiple tasks and append them to a new list.
        Function required for Airflow version 2.10.0 or later
        (https://github.com/apache/airflow/issues/41983).
        """
        search_results = []
        for counter in range(1, num_searches + 1):
            search_results.append(
                context["ti"].xcom_pull(task_ids=f"exec_searchs.exec_search_{counter}")
            )

        return search_results

    def has_matches(self, num_searches: int, skip_null: bool, **context) -> str:
        """Check if search has matches and return to skip notification or not"""

        if skip_null:
            search_results = self.get_xcom_pull_tasks(
                num_searches=num_searches, **context
            )

            skip_notification = True

            for search in search_results:
                items = ["contains" for k, v in search["result"].items() if v]
                if items:
                    skip_notification = False
            return "skip_notification" if skip_notification else "send_notification"
        else:
            return "send_notification"

    def select_terms_from_db(self, sql: str, conn_id: str):
        """Queries the `sql` and return the list of terms that will be
        used later in the DOU search. The first column of the select
        must contain the terms to be searched. The second column, which
        is optional, is a classifier that will be used to group and sort
        the email report and the generated CSV.
        """
        conn_type = BaseHook.get_connection(conn_id).conn_type
        if conn_type == "mssql":
            db_hook = MsSqlHook(conn_id)
        elif conn_type in ("postgresql", "postgres"):
            db_hook = PostgresHook(conn_id)
        else:
            raise Exception("Tipo de banco de dados não suportado: ", conn_type)

        terms_df = db_hook.get_pandas_df(sql)
        # Remove unnecessary spaces and change null for ''
        terms_df = terms_df.applymap(lambda x: str.strip(x) if pd.notnull(x) else "")

        return terms_df.to_json(orient="columns")

    def send_notification(
        self, num_searches: int, specs: DAGConfig, report_date: str, **context
    ) -> str:
        """Send user notification using class Notifier"""
        search_report = self.get_xcom_pull_tasks(num_searches=num_searches, **context)

        notifier = Notifier(specs)

        notifier.send_notification(search_report=search_report, report_date=report_date)

    def create_dag(self, specs: DAGConfig, config_file: str) -> DAG:
        """Creates the DAG object and tasks

        Depending on configuration it adds an extra prior task to query
        the term_list from a database
        """
        # Prepare the markdown documentation
        doc_md = self.prepare_doc_md(specs, config_file) if specs.doc_md else None
        # DAG parameters
        default_args = {
            "owner": ",".join(specs.owner),
            "start_date": datetime(2021, 10, 18),
            "depends_on_past": False,
            "retries": 10,
            "retry_delay": timedelta(minutes=20),
            "on_retry_callback": self.on_retry_callback,
            "on_failure_callback": self.on_failure_callback,
        }

        schedule = self._update_schedule(specs)
        dag = DAG(
            specs.id,
            default_args=default_args,
            schedule=schedule,
            description=specs.description,
            doc_md=doc_md,
            catchup=False,
            params={
                "trigger_date": Param(
                    default=date.today().isoformat(), type="string", format="date"
                )
            },
            tags=list(specs.tags),
        )

        with dag:

            with TaskGroup(group_id="exec_searchs") as tg_exec_searchs:

                searches = specs.search

                for counter, subsearch in enumerate(searches, 1):

                    # are terms to be fetched from a database?
                    terms_come_from_db: bool = isinstance(
                        subsearch.terms, FetchTermsConfig
                    ) and getattr(subsearch.terms, "from_db_select", None)

                    # determine the terms list
                    term_list = []
                    # is it a directly defined list of terms or is it a
                    # configuration for fetching terms from a data source?
                    if isinstance(subsearch.terms, list):
                        term_list = subsearch.terms
                    elif terms_come_from_db:
                        select_terms_from_db_task = PythonOperator(
                            task_id=f"select_terms_from_db_{counter}",
                            python_callable=self.select_terms_from_db,
                            op_kwargs={
                                "sql": subsearch.terms.from_db_select.sql,
                                "conn_id": subsearch.terms.from_db_select.conn_id,
                            },
                        )
                        term_list = (
                            "{{ ti.xcom_pull(task_ids='exec_searchs.select_terms_from_db_"
                            + str(counter)
                            + "') }}"
                        )

                    exec_search_task = PythonOperator(
                        task_id=f"exec_search_{counter}",
                        python_callable=self.perform_searches,
                        op_kwargs={
                            "header": subsearch.header,
                            "sources": subsearch.sources,
                            "territory_id": subsearch.territory_id,
                            "term_list": term_list,
                            "dou_sections": subsearch.dou_sections,
                            "search_date": subsearch.date,
                            "field": subsearch.field,
                            "is_exact_search": subsearch.is_exact_search,
                            "ignore_signature_match": subsearch.ignore_signature_match,
                            "force_rematch": subsearch.force_rematch,
                            "full_text": subsearch.full_text,
                            "use_summary": subsearch.use_summary,
                            "department": subsearch.department,
                            "pubtype": subsearch.pubtype,
                            "result_as_email": result_as_html(specs),
                        },
                    )

                    if terms_come_from_db:
                        # pylint: disable=pointless-statement
                        select_terms_from_db_task >> exec_search_task

            has_matches_task = BranchPythonOperator(
                task_id="has_matches",
                python_callable=self.has_matches,
                op_kwargs={
                    "num_searches": len(searches),
                    "skip_null": specs.report.skip_null,
                },
            )

            skip_notification_task = EmptyOperator(task_id="skip_notification")

            send_notification_task = PythonOperator(
                task_id="send_notification",
                python_callable=self.send_notification,
                op_kwargs={
                    "num_searches": len(searches),
                    "specs": specs,
                    "report_date": template_ano_mes_dia_trigger_local_time,
                },
            )

            # pylint: disable=pointless-statement
            tg_exec_searchs >> has_matches_task

            has_matches_task >> [send_notification_task, skip_notification_task]

        return dag


# # Run dag generation
DouDigestDagGenerator().generate_dags()
