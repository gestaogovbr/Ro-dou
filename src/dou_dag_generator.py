"""
Dynamic DAG generator with YAML config system to create DAG which
searches terms in the Gazzete [Diário Oficial da União-DOU] and send it
by email to the  provided `recipient_emails` list. The DAGs are
generated by YAML config files at `dag_confs` folder.

TODO:
[] - Choose the title suffix for the email using the configuration.
"""
import ast
import json
import logging
import os
import sys
import textwrap
from datetime import datetime, timedelta, date
from typing import Dict, List, Optional, Union
from functools import reduce

import pandas as pd
from airflow import DAG, Dataset
from airflow.models import Variable
from airflow.models.param import Param
from airflow.utils.task_group import TaskGroup
from airflow.hooks.base import BaseHook
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import BranchPythonOperator, PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.slack.notifications.slack import SlackNotifier
from airflow.timetables.datasets import DatasetOrTimeSchedule
from airflow.timetables.trigger import CronTriggerTimetable

try:
    from airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook
except ImportError:
    MsSqlHook = None

sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))
from utils.date import get_trigger_date, template_ano_mes_dia_trigger_local_time
from notification.notifier import Notifier
from parsers import DAGConfig, YAMLParser
from schemas import FetchTermsConfig
from searchers import BaseSearcher, DOUSearcher, QDSearcher, INLABSSearcher
from airflow.utils.email import send_email

SearchResult = Dict[str, Dict[str, Dict[str, List[dict]]]]


def merge_results(*dicts: SearchResult) -> SearchResult:
    """
    Merge multiple dictionaries and sum/concatenate values of common keys,
    including nested dictionaries.
    """

    def merge_two(dict1, dict2):
        merged = {}

        # Combine keys from both dictionaries
        all_keys = set(dict1) | set(dict2)

        for key in all_keys:
            value1 = dict1.get(key)
            value2 = dict2.get(key)

            if isinstance(value1, dict) and isinstance(value2, dict):
                # If both values are dictionaries, merge them recursively
                merged[key] = merge_results(value1, value2)
            elif isinstance(value1, dict) or isinstance(value2, dict):
                # If one of the values is a dict, prefer the dict (override with dict)
                merged[key] = value1 if isinstance(value1, dict) else value2
            else:
                # Sum or concatenate the values, treating None or missing keys as 0
                merged[key] = (value1) + (value2)

        return merged

    # Pre-processing step: Filter out dictionaries where the first key's value is empty
    filtered_dicts = []

    for d in dicts:
        if d:  # Ensure the dictionary is not empty
            first_key = next(iter(d))
            first_value = d.get(first_key)
            if first_value:  # Only include if the first key's value is non-empty
                filtered_dicts.append(d)

    # If no dictionaries remain after filtering, return an empty dictionary
    if not filtered_dicts:
        return {}

    # Reduce the list of dictionaries by merging them two at a time
    return reduce(merge_two, filtered_dicts)


def result_as_html(specs: DAGConfig) -> bool:
    """Use HTML results only for emails"""
    return bool(not (specs.report.discord or specs.report.slack))


class DouDigestDagGenerator:
    """
    YAML based Generator of DAGs that digests the DOU (gazette) daily
    publication and send email report listing all documents matching
    pré-defined keywords. It's also possible to fetch keywords from the
    database.
    """

    YAMLS_DIR = os.getenv("RO_DOU__DAG_CONF_DIR")

    if YAMLS_DIR is None:
        raise EnvironmentError("Environment variable RO_DOU__DAG_CONF_DIR not found!")

    YAMLS_DIR_LIST = [dag_confs for dag_confs in YAMLS_DIR.split(":")]
    SLACK_CONN_ID = "slack_notify_rodou_dagrun"
    DEFAULT_SCHEDULE = "0 5 * * *"

    parser = YAMLParser
    searchers: Dict[str, BaseSearcher]

    def __init__(self):
        self.searchers = {
            "DOU": DOUSearcher(),
            "QD": QDSearcher(),
            "INLABS": INLABSSearcher(),
        }       

        self.on_failure_callback = self._notify_on_failure
        self.on_retry_callback = None

    def _notify_on_failure(self, specs: DAGConfig, context):
        """Function called when the task fails to send a notification to admin"""
        try:
           
            task_instance = context.get('task_instance')
            dag_run = context.get('dag_run')
            exception = context.get('exception')

            if not task_instance or not dag_run:
                logging.error("Missing required context: task_instance or dag_run")
                return

            # Determina lista de emails para notificação
            email_list = []
            
            if specs.callback and specs.callback.on_failure_callback:
                # Usa emails configurados no .yaml
                email_list = specs.callback.on_failure_callback
            else:
                # Usa email admin padrão
                try:
                    email_admin = Variable.get('email_admin', default_var=None)
                    if email_admin:
                        email_list = [email_admin]                        
                    else:
                        logging.warning("No email_admin variable found in Airflow")
                except Exception as e:
                    logging.error(f"Error getting email_admin variable: {str(e)}")

            if not email_list:
                logging.warning("No email configured for failure notification (neither callback nor email_admin variable)")

            # Envia email se houver destinatários
            if email_list:
                try:
                    execution_date_str = dag_run.execution_date.strftime("%d/%m/%Y %H:%M") if dag_run.execution_date else "N/A"

                    send_email(
                        to=email_list,
                        subject=f'Falha na DAG {dag_run.dag_id}',
                        html_content=f'''
                        <h3>Falha detectada</h3>
                        <p><strong>DAG:</strong> {dag_run.dag_id}</p>
                        <p><strong>Task:</strong> {task_instance.task_id}</p>
                        <p><strong>Estado:</strong> {task_instance.state}</p>
                        <p><strong>Data de execução:</strong> {execution_date_str}</p>
                        <p><strong>Exceção:</strong> {exception}</p>
                        <p><strong>Log:</strong> <a href="{task_instance.log_url}" target="_blank">Ver log completo</a></p>
                        '''
                    )
                    logging.info(f"SUCCESS: Failure notification email sent to: {email_list}")
                except Exception as e:
                    logging.error(f"ERROR: Failed to send failure notification email: {str(e)}", exc_info=True)

            # Tenta enviar notificação via Slack (se disponível)
            try:
                conn = BaseHook.get_connection(self.SLACK_CONN_ID)
                description = json.loads(conn.description)
                slack_notifier = SlackNotifier(
                    slack_conn_id=self.SLACK_CONN_ID,
                    text=(
                        ":bomb: *Falha na DAG*"
                        f"\n*DAG:* `{dag_run.dag_id}`"
                        f"\n*Task:* `{task_instance.task_id}`"
                        f"\n*State:* `{task_instance.state}`"
                        f"\n*Data de execução:* {dag_run.execution_date.strftime('%d/%m/%Y %H:%M') if dag_run.execution_date else 'N/A'}"
                        f"\n*Exception:* {exception}"
                        f"\n*Log:* <{task_instance.log_url}|Ver log completo>"
                    ),
                    channel=description["channel"],
                )
                slack_notifier.notify(context)                
            except Exception as e:
                logging.error(f"Slack notification not sent: {str(e)}")

        except Exception as e:
            logging.error(f"Error in _notify_on_failure: {str(e)}", exc_info=True)
    
    def prepare_ai_context(self, num_searches: int, **context) -> dict:
        """Prepara dados estruturados para processamento pela IA"""
        search_results = self.get_xcom_pull_tasks(num_searches=num_searches, **context)
        summary = context["ti"].xcom_pull(task_ids="summarize_publications")

        publications_for_ai = []

        for search in search_results:
            if not search or "result" not in search:
                continue

            result = search["result"]
            header = search.get("header", "")

            for source, sections in result.items():
                # source = "single_group"
                # sections = {"dados abertos": {...}, "governo aberto": {...}, ...}
                
                if not isinstance(sections, dict):
                    continue
                    
                for section_name, section_data in sections.items():
                    # section_name = "dados abertos"
                    # section_data = {"single_department": [...]}
                    
                    if not isinstance(section_data, dict):
                        continue
                        
                    # Procura por "single_department" dentro de section_data
                    for key, value in section_data.items():
                        # key = "single_department"
                        # value = lista de publicações
                        
                        if isinstance(value, list):
                            for pub in value:
                                if isinstance(pub, dict):
                                    publications_for_ai.append({
                                        "source": source,  # "single_group"
                                        "section": section_name,  # "dados abertos"
                                        "header": header,  # "Pesquisa no DOU"
                                        "title": pub.get("title", ""),
                                        "abstract": pub.get("abstract", ""),
                                        "content": pub.get("abstract", "")[:1000],
                                        "date": pub.get("date", ""),
                                        "hierarchy": pub.get("hierarchyStr", ""),
                                        "type": pub.get("arttype", ""),
                                        "href": pub.get("href", ""),
                                    })

        logging.info(f"Total publications prepared for AI: {len(publications_for_ai)}")

        ai_context = {
            "summary_stats": summary,
            "publications": publications_for_ai[:30],
            "report_date": get_trigger_date(context, local_time=True)
        }
        
        return ai_context

    def generate_ai_summary(self, num_searches: int, **context) -> str:
        """Usa AI para gerar resumo inteligente das publicações"""
        try:
            # Verifica se IA está habilitada
            llm_enabled = Variable.get("LLM_ENABLED", default_var="false").lower() == "true"
            
            if not llm_enabled:
                logging.info("LLM disabled (LLM_ENABLED=false), skipping AI summary generation")
                return None
            
            ai_context = context["ti"].xcom_pull(task_ids="prepare_ai_context")
            
            if not ai_context:
                logging.warning("AI context not found, returning empty summary")
                return "Resumo IA não disponível (contexto vazio)."
            
            summary_stats = ai_context.get('summary_stats', {})
            publications = ai_context.get('publications', [])
            report_date = ai_context.get('report_date', 'data desconhecida')
            
            if not publications:
                logging.info("No publications to summarize")
                return "Não há publicações para resumir."
            
            # Monta o prompt para a IA
            prompt = self._build_ai_prompt(summary_stats, publications, report_date)
            
            # Chama a API de IA
            ai_summary = self._call_ai_api(prompt)
            
            logging.info(f"AI summary generated successfully ({len(ai_summary)} characters)")
            
            return ai_summary
            
        except Exception as e:
            logging.error(f"Error generating AI summary: {str(e)}", exc_info=True)
            return f"Erro ao gerar resumo IA: {str(e)}"

    def _build_ai_prompt(self, summary_stats: dict, publications: list, report_date: str) -> str:
        """Constrói o prompt para a IA"""
        prompt = f"""Você é um assistente especializado em análise de publicações do Diário Oficial da União (DOU).

Analise as seguintes publicações do Diário Oficial de {report_date}:

ESTATÍSTICAS GERAIS:
- Total de publicações encontradas: {summary_stats.get('total_publications', 0)}
- Fontes consultadas: {', '.join(summary_stats.get('publications_by_source', {}).keys())}
- Buscas com resultados: {summary_stats.get('searches_with_results', 0)}/{summary_stats.get('total_searches', 0)}

DISTRIBUIÇÃO POR FONTE:
"""
        for source, count in summary_stats.get('publications_by_source', {}).items():
            prompt += f"- {source}: {count} publicações\n"
        
        prompt += f"\n\nPUBLICAÇÕES PARA ANÁLISE (amostra de {len(publications)}):\n\n"
        
        # Adiciona detalhes das publicações (limitado a 15 para não estourar o contexto)
        for idx, pub in enumerate(publications[:15], 1):
            prompt += f"{idx}. [{pub.get('source', 'N/A')}] {pub.get('section', 'N/A')}\n"
            prompt += f"   Título: {pub.get('title', 'Sem título')}\n"
            if pub.get('abstract'):
                prompt += f"   Resumo: {pub.get('abstract')[:300]}...\n"
            elif pub.get('content'):
                prompt += f"   Conteúdo: {pub.get('content')[:300]}...\n"
            prompt += "\n"
        
        prompt += """
Com base nas informações acima, gere um RESUMO EXECUTIVO em português que contenha:

1. **Principais Temas Identificados**: Liste os 3-5 temas mais recorrentes nas publicações
2. **Publicações Mais Relevantes**: Destaque 2-3 publicações que merecem atenção especial
3. **Tendências ou Padrões**: Identifique padrões interessantes (ex: aumento de publicações de um órgão específico)
4. **Alertas Importantes**: Se houver algo que demande ação imediata ou atenção especial

Formato: Texto claro, conciso e em português brasileiro, com até 500 palavras.
Use markdown para formatação (negrito, listas, etc.).
"""
        
        return prompt

    def _call_ai_api(self, prompt: str) -> str:
        """Chama a API de IA configurada (Gemini, OpenAI ou outro)"""
        try:
            llm_provider = Variable.get("LLM_PROVIDER", default_var="disabled").lower()
            
            if llm_provider == "disabled":
                logging.info("LLM provider is 'disabled', returning placeholder")
                return self._generate_placeholder_summary()
            
            api_key = Variable.get("LLM_API_KEY", default_var=None)
            
            if not api_key:
                logging.error("LLM_API_KEY not configured")
                return "Erro: LLM_API_KEY não configurada nas variáveis do Airflow"
            
            if llm_provider == "gemini":
                return self._call_gemini_api(prompt, api_key)
            elif llm_provider == "openai":
                return self._call_openai_api(prompt, api_key)
            else:
                logging.error(f"Unsupported LLM provider: {llm_provider}")
                return f"Erro: Provedor LLM não suportado: {llm_provider}"
                
        except Exception as e:
            logging.error(f"Error calling AI API: {str(e)}", exc_info=True)
            return f"Erro ao chamar API de IA: {str(e)}"

    def _call_gemini_api(self, prompt: str, api_key: str) -> str:
        """Chama a API do Google Gemini"""
        try:
            import google.generativeai as genai

            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-2.5-flash')
            
            response = model.generate_content(prompt)
            
            if response and response.text:
                return response.text
            else:
                logging.warning("Gemini returned empty response")
                return "Resumo não pôde ser gerado (resposta vazia do Gemini)"
                
        except ImportError:
            logging.error("google-generativeai package not installed")
            return "Erro: Pacote google-generativeai não instalado. Execute: pip install google-generativeai"
        except Exception as e:
            logging.error(f"Gemini API error: {str(e)}", exc_info=True)
            return f"Erro ao chamar Gemini API: {str(e)}"

    def _call_openai_api(self, prompt: str, api_key: str) -> str:
        """Chama a API da OpenAI (GPT)"""
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=api_key)
            
            response = client.chat.completions.create(
                model="gpt-4",  # ou "gpt-3.5-turbo" para economizar
                messages=[
                    {"role": "system", "content": "Você é um assistente especializado em análise de publicações oficiais brasileiras."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=1000
            )
            
            if response and response.choices and len(response.choices) > 0:
                return response.choices[0].message.content
            else:
                logging.warning("OpenAI returned empty response")
                return "Resumo não pôde ser gerado (resposta vazia da OpenAI)"
                
        except ImportError:
            logging.error("openai package not installed")
            return "Erro: Pacote openai não instalado. Execute: pip install openai"
        except Exception as e:
            logging.error(f"OpenAI API error: {str(e)}", exc_info=True)
            return f"Erro ao chamar OpenAI API: {str(e)}"

    def _generate_placeholder_summary(self) -> str:
        """Gera um placeholder quando a IA está desabilitada"""
        return """**Resumo IA - Modo Desabilitado**

A funcionalidade de resumo inteligente está atualmente desabilitada.

Para habilitar:
1. Configure LLM_ENABLED=true
2. Configure LLM_PROVIDER (gemini ou openai)
3. Configure LLM_API_KEY com sua chave de API

Status atual das variáveis:
- LLM_ENABLED: Verificar no Airflow
- LLM_PROVIDER: Verificar no Airflow
- LLM_API_KEY: Verificar se está configurada
"""

    @staticmethod
    def prepare_doc_md(specs: DAGConfig, config_file: str) -> str:
        """Prepares the markdown documentation for a dag.

        Args:
            specs (DAGConfig): A DAG configuration object.
            config_file (str): The name of a DAG config file.

        Returns:
            str: The DAG documentation in markdown format.
        """
        config = specs.model_dump()
        # options that won't show in the "DAG Docs"
        del config["description"]
        del config["doc_md"]
        doc_md = specs.doc_md + textwrap.dedent(
            f"""

            **Configuração da dag definida no arquivo `{config_file}`**:


            """
        )
        for key, value in config.items():
            doc_md += f"\n**{key.replace('_', ' ').capitalize()}**\n"

            if isinstance(value, (list, set)):
                doc_md += "\n" + "\n".join(f"- {str(item)}" for item in value) + "\n"
            else:
                doc_md += "\n" + f"- {str(value)}\n" + "\n"

        return doc_md

    @staticmethod
    def _hash_dag_id(dag_id: str, size: int) -> int:
        """Hashes the `dag_id` into a integer between 0 and `size`"""
        buffer = 0
        for _char in dag_id:
            buffer += ord(_char)
        try:
            _hash = buffer % size
        except ZeroDivisionError:
            raise ValueError("`size` deve ser maior que 0.")
        return _hash

    def _get_safe_schedule(self, specs: DAGConfig, default_schedule: str) -> str:
        """Return a new value of `schedule`, randomazing the
        execution minute accorgind to `dag_id`, if the dag use the
        schedule standard. Apply a function of hash to string
        dag_id to return a value between 0 and 60, wich defines the execution minute.
        """

        schedule = default_schedule
        id_based_minute = self._hash_dag_id(specs.id, 60)
        schedule_without_min = " ".join(schedule.split(" ")[1:])
        schedule = f"{id_based_minute} {schedule_without_min}"

        return schedule

    def _update_schedule_with_dataset(
        self, dataset: str, schedule: str, is_default_schedule: bool
    ) -> Union[Dataset, DatasetOrTimeSchedule]:
        """If a dataset is provide, the schedule will be update
        to make execution conditional on the Dataset or
        DatasetOrTimeSchedule
        (if the value of schedule is specified in the YAML information).
        """
        if not is_default_schedule:
            return DatasetOrTimeSchedule(
                timetable=CronTriggerTimetable(
                    schedule, timezone=os.getenv("AIRFLOW__CORE__DEFAULT_TIMEZONE")
                ),
                datasets=[Dataset(dataset)],
            )
        return [Dataset(dataset)]

    def _update_schedule(
        self, specs: DAGConfig
    ) -> Union[str, Union[Dataset, DatasetOrTimeSchedule]]:
        """The DAG will update the value of schedule to
        the default value or to a Dataset, if that option is specified.
        """
        schedule = specs.schedule

        if schedule is None:
            schedule = self._get_safe_schedule(
                specs=specs, default_schedule=self.DEFAULT_SCHEDULE
            )
            is_default_schedule = True
        else:
            is_default_schedule = False

        if specs.dataset is not None:
            schedule = self._update_schedule_with_dataset(
                dataset=specs.dataset,
                schedule=schedule,
                is_default_schedule=is_default_schedule,
            )

        return schedule

    def generate_dags(self):
        """Iterates over the YAML files and creates all dags"""

        files_list = []

        for directory in self.YAMLS_DIR_LIST:
            for dirpath, _, filenames in os.walk(directory):
                for filename in filenames:
                    if any(ext in filename for ext in [".yaml", ".yml"]):
                        files_list.extend([os.path.join(dirpath, filename)])

        for filepath in files_list:
            dag_specs = self.parser(filepath).parse()
            dag_id = dag_specs.id
            globals()[dag_id] = self.create_dag(dag_specs, filepath)

    def _parse_term_list(self, term_list):
        """Converte term_list para lista, tratando aspas duplas extras."""
        if not isinstance(term_list, str):
            return term_list
        
        # Remove espaços
        term_list = term_list.strip()
        
        # Tenta converter até 2 vezes (para casos de serialização dupla)
        for _ in range(2):
            if term_list.startswith(('[', '"[', "'[")):
                try:
                    term_list = ast.literal_eval(term_list)
                    if isinstance(term_list, list):
                        return term_list
                except (ValueError, SyntaxError):
                    break
        
        return term_list

    def perform_searches(
        self,
        header,
        sources,
        territory_id,
        term_list,
        dou_sections: List[str],
        search_date,
        field,
        is_exact_search: Optional[bool],
        ignore_signature_match: Optional[bool],
        force_rematch: Optional[bool],
        full_text: Optional[bool],
        text_length: Optional[int],
        use_summary: Optional[bool],
        result_as_email: Optional[bool],
        department: List[str],
        department_ignore: List[str],
        pubtype: List[str],
        excerpt_size: Optional[int],
        number_of_excerpts: Optional[int],
        **context,
    ) -> dict:
        """Performs the search in each source and merge the results"""        
        if "DOU" in sources:
            dou_result = self.searchers["DOU"].exec_search(
                term_list=term_list,
                dou_sections=dou_sections,
                search_date=search_date,
                field=field,
                is_exact_search=is_exact_search,
                ignore_signature_match=ignore_signature_match,
                force_rematch=force_rematch,
                department=department,
                department_ignore=department_ignore,
                pubtype=pubtype,
                reference_date=get_trigger_date(context, local_time=True),
            )
        elif "INLABS" in sources:                         
            terms = self._parse_term_list(term_list)
            inlabs_result = self.searchers["INLABS"].exec_search(
                terms=terms,
                dou_sections=dou_sections,
                search_date=search_date,
                department=department,
                department_ignore=department_ignore,
                ignore_signature_match=ignore_signature_match,
                full_text=full_text,
                text_length=text_length,
                use_summary=use_summary,
                pubtype=pubtype,
                reference_date=get_trigger_date(context, local_time=True),
            )

        if "QD" in sources:
            qd_result = self.searchers["QD"].exec_search(
                territory_id=territory_id,
                term_list=term_list,
                is_exact_search=is_exact_search,
                reference_date=get_trigger_date(context, local_time=True),
                excerpt_size=excerpt_size,
                number_of_excerpts=number_of_excerpts,
                result_as_email=result_as_email,
            )

        if "DOU" in sources and "QD" in sources:
            result = merge_results(qd_result, dou_result)
        elif "INLABS" in sources and "QD" in sources:
            result = merge_results(qd_result, inlabs_result)
        elif "DOU" in sources:
            result = dou_result
        elif "INLABS" in sources:
            result = inlabs_result
        else:
            result = qd_result

        # Add more specs info
        search_dict = {}
        search_dict["result"] = result
        search_dict["header"] = header
        search_dict["department"] = department
        search_dict["department_ignore"] = department_ignore
        search_dict["pubtype"] = pubtype

        return search_dict

    def get_xcom_pull_tasks(self, num_searches, **context):
        """Retrieve XCom values from multiple tasks and append them to a new list.
        Function required for Airflow version 2.10.0 or later
        (https://github.com/apache/airflow/issues/41983).
        """
        search_results = []
        for counter in range(1, num_searches + 1):
            search_results.append(
                context["ti"].xcom_pull(task_ids=f"exec_searchs.exec_search_{counter}")
            )

        return search_results

    def has_matches(self, num_searches: int, skip_null: bool, **context) -> str:
        """Check if search has matches and return to skip notification or not"""

        if skip_null:
            search_results = self.get_xcom_pull_tasks(
                num_searches=num_searches, **context
            )

            skip_notification = True

            for search in search_results:
                if search and search.get("result"):
                    items = ["contains" for k, v in search["result"].items() if v]
                    if items:
                        skip_notification = False
            
            return "skip_notification" if skip_notification else "summarize_publications"
        else:
            return "summarize_publications"

    def summarize_publications(self, num_searches: int, **context) -> dict:
        """
        Summarizes publications from all searches before sending notification.
        Returns statistics and prepares data for potential AI enhancement.
        """
        search_results = self.get_xcom_pull_tasks(num_searches=num_searches, **context)

        summary = {
            "total_searches": num_searches,
            "searches_with_results": 0,
            "total_publications": 0,
            "publications_by_source": {},
            "publications_by_section": {},
            "publications_by_header": {},
            "has_content": False
        }
        logging.info(f"Searching result {search_results} searches")

        for idx, search in enumerate(search_results, 1):
            if not search or not search.get("result"):
                continue
                
            result = search["result"]
            header = search.get("header", f"Search {idx}")
            
            logging.info(f"DEBUG: Processing search {idx} with header '{header}'")
            logging.info(f"DEBUG: Result keys: {list(result.keys())}")

            publication_count = 0
        
            for source, categories in result.items():
                logging.info(f"DEBUG: Source: {source}")
            
                if source not in summary["publications_by_source"]:
                    summary["publications_by_source"][source] = 0
                
                for category, content in categories.items():
                    logging.info(f"DEBUG: Category: {category}, Type of content: {type(content)}")
                    if isinstance(content, dict) and 'single_department' in content:
                        pubs = content['single_department']
                        if isinstance(pubs, list):
                            pub_count = len(pubs)
                            summary["publications_by_source"][source] += pub_count
                            summary["total_publications"] += pub_count
                            publication_count += pub_count
                                
                            # Registrar por seção
                            section_key = f"{source}_{category}"
                            if section_key not in summary["publications_by_section"]:
                                summary["publications_by_section"][section_key] = 0
                            summary["publications_by_section"][section_key] += pub_count
                            
                            logging.info(f"DEBUG: Found {pub_count} publications in {category}")
                        else:
                            logging.warning(f"DEBUG: 'single_department' is not a list in {category}")
                    else:
                        logging.warning(f"DEBUG: Unexpected content structure in {category}: {type(content)}")


            if publication_count > 0:
                summary["searches_with_results"] += 1
                summary["has_content"] = True
                
                if header not in summary["publications_by_header"]:
                    summary["publications_by_header"][header] = 0
                summary["publications_by_header"][header] += publication_count

                logging.info(f"DEBUG: Search {idx} has {publication_count} publications")
            else:
                logging.info(f"DEBUG: Search {idx} has 0 publications")
        
        logging.info(
            f"Summary generated: {summary['total_publications']} publications "
            f"from {summary['searches_with_results']}/{num_searches} searches"
        )
        logging.info(f"Publications by source: {summary['publications_by_source']}")
        logging.info(f"Publications by header: {summary['publications_by_header']}")
        
        return summary
        
    def send_notification(
        self, num_searches: int, specs: DAGConfig, report_date: str, **context
    ) -> str:
        """Send user notification using class Notifier"""
        search_report = self.get_xcom_pull_tasks(num_searches=num_searches, **context)
        
        # Recupera o summary se disponível
        summary = context["ti"].xcom_pull(task_ids="summarize_publications")
        # Recupera o resumo da IA se disponível
        ai_summary = context["ti"].xcom_pull(task_ids="generate_ai_summary")
        
        notifier = Notifier(specs)

        logging.info(f"Summary: {summary}")
        logging.info(f"IA Summary: {ai_summary}")
        
        # Passa o summary e ai_summary para o notifier
        notifier.send_notification(
            search_report=search_report, 
            report_date=report_date
        )

    def select_terms_from_airflow_variable(self, variable: str):
        """
        Retrieves and processes a list of terms from an Apache Airflow variable.

        This function searches for a specific Airflow variable and converts it into a list
        of terms, supporting both JSON and line-delimited text formats.

        Arguments:
        variable (str): Name of the Airflow variable to be retrieved.

        Returns:
        list: List of terms extracted from the Airflow variable.

        Raises:
        KeyError: When a specified variable was not found in Airflow.

        Examples:
        >>> # For an Airflow variable containing JSON: ["term1", "term2", "term3"]
        >>> terms = self.select_terms_from_airflow_variable("my_json_list")
        >>> print(terms)
        ['term1', 'term2', 'term3']

        >>> # For a variable An Airflow variable containing text separated by lines:
        >>>#term1
        >>>#term2
        >>>#term3
        >>> terms = self.select_terms_from_airflow_variable("my_text_list")
        >>> print (terms)
        ['term1', 'term2', 'term3']

        Note:
        - If the variable value is a list (JSON), it will be parsed with json.loads()
        - Otherwise, it will be treated as a string and split by line breaks
        - Useful for configuring dynamic lists using Airflow variables
        """        

        term_list = []
        var_name = variable
        
        try:
            var_value = Variable.get(var_name)
            # Se já é uma lista, retorna direto
            if isinstance(var_value, list):
                return var_value
        
            if isinstance(var_value, str):
                if var_value.strip().startswith('['):
                    return ast.literal_eval(var_value)
                else:
                    # Trata como texto separado por linhas
                    return var_value.splitlines()                    
            return term_list

        except (KeyError):
            raise KeyError(
                f"Airflow variable {var_name} not found."
            )

    def select_terms_from_db(self, sql: str, conn_id: str):
        """Queries the `sql` and return the list of terms that will be
        used later in the DOU search. The first column of the select
        must contain the terms to be searched. The second column, which
        is optional, is a classifier that will be used to group and sort
        the email report and the generated CSV.
        """
        conn_type = BaseHook.get_connection(conn_id).conn_type
        if conn_type == "mssql":
            if MsSqlHook is None:
                raise RuntimeError("MsSqlHook indisponível: instale 'apache-airflow-providers-microsoft-mssql' para usar recursos MSSQL.")
            db_hook = MsSqlHook(conn_id)
        elif conn_type in ("postgresql", "postgres"):
            db_hook = PostgresHook(conn_id)
        else:
            raise Exception("Tipo de banco de dados não suportado: ", conn_type)

        terms_df = db_hook.get_pandas_df(sql)
        # Remove unnecessary spaces and change null for ''
        terms_df = terms_df.applymap(lambda x: str.strip(x) if pd.notnull(x) else "")
      
        return terms_df.to_json(orient="columns")

    def create_dag(self, specs: DAGConfig, config_file: str) -> DAG:
        """Creates the DAG object and tasks

        Depending on configuration it adds an extra prior task to query
        the term_list from a database
        """
        # Prepare the markdown documentation
        doc_md = self.prepare_doc_md(specs, config_file) if specs.doc_md else None
        # DAG parameters
        default_args = {
            "owner": ",".join(specs.owner),
            "start_date": datetime(2021, 10, 18),
            "depends_on_past": False,
            "retries": 10,
            "retry_delay": timedelta(minutes=2),
            "on_retry_callback": self.on_retry_callback,
            "on_failure_callback": lambda context: self._notify_on_failure(specs, context),
        }

        schedule = self._update_schedule(specs)
        dag = DAG(
            specs.id,
            default_args=default_args,
            schedule=schedule,
            description=specs.description,
            doc_md=doc_md,
            catchup=False,
            params={
                "trigger_date": Param(
                    default=date.today().isoformat(), type="string", format="date"
                )
            },
            tags=list(specs.tags),
        )

        with dag:

            with TaskGroup(group_id="exec_searchs") as tg_exec_searchs:

                searches = specs.search

                for counter, subsearch in enumerate(searches, 1):                   
                    # Verify if the terms were fetched from the database
                    terms_come_from_db: bool = isinstance(
                        subsearch.terms, FetchTermsConfig
                    ) and getattr(subsearch.terms, "from_db_select", None)

                    terms_come_from_airflow_variable: bool = isinstance(
                        subsearch.terms, FetchTermsConfig
                    ) and getattr(subsearch.terms, "from_airflow_variable", None)

                    # determine the terms list
                    term_list = []
                    # is it a directly defined list of terms or is it a
                    # configuration for fetching terms from a data source?
                    if subsearch.terms is None:
                        # No specific terms - will search all publications with filters
                        term_list = None
                    elif isinstance(subsearch.terms, list):
                        term_list = subsearch.terms
                    elif terms_come_from_airflow_variable:
                        select_terms_from_airflow_variable_task = PythonOperator(
                            task_id=f"select_terms_from_airflow_variable_{counter}",
                            python_callable=self.select_terms_from_airflow_variable,
                            op_kwargs={
                                "variable": subsearch.terms.from_airflow_variable
                            }
                        )
                        term_list = (
                            "{{ ti.xcom_pull(task_ids='exec_searchs.select_terms_from_airflow_variable_"
                            + str(counter)
                            + "') }}"
                        )
                        
                    elif terms_come_from_db:
                        select_terms_from_db_task = PythonOperator(
                            task_id=f"select_terms_from_db_{counter}",
                            python_callable=self.select_terms_from_db,
                            op_kwargs={
                                "sql": subsearch.terms.from_db_select.sql,
                                "conn_id": subsearch.terms.from_db_select.conn_id,
                            },
                        )
                        term_list = (
                            "{{ ti.xcom_pull(task_ids='exec_searchs.select_terms_from_db_"
                            + str(counter)
                            + "') }}"
                        )
                                           
                    exec_search_task = PythonOperator(
                        task_id=f"exec_search_{counter}",
                        python_callable=self.perform_searches,
                        op_kwargs={
                            "header": subsearch.header,
                            "sources": subsearch.sources,
                            "territory_id": subsearch.territory_id,
                            "term_list": term_list,
                            "dou_sections": subsearch.dou_sections,
                            "search_date": subsearch.date,
                            "field": subsearch.field,
                            "is_exact_search": subsearch.is_exact_search,
                            "ignore_signature_match": subsearch.ignore_signature_match,
                            "force_rematch": subsearch.force_rematch,
                            "full_text": subsearch.full_text,
                            "text_length": subsearch.text_length,
                            "use_summary": subsearch.use_summary,
                            "department": subsearch.department,
                            "department_ignore": subsearch.department_ignore,
                            "pubtype": subsearch.pubtype,
                            "excerpt_size": subsearch.excerpt_size,
                            "number_of_excerpts": subsearch.number_of_excerpts,
                            "result_as_email": result_as_html(specs),
                        },
                    )

                    if terms_come_from_airflow_variable:
                        select_terms_from_airflow_variable_task >> exec_search_task

                    if terms_come_from_db:
                        select_terms_from_db_task >> exec_search_task
                        
            has_matches_task = BranchPythonOperator(
                task_id="has_matches",
                python_callable=self.has_matches,
                op_kwargs={
                    "num_searches": len(searches),
                    "skip_null": specs.report.skip_null,
                },
            )

            skip_notification_task = EmptyOperator(task_id="skip_notification")

            # Task: Summarize publications
            summarize_publications_task = PythonOperator(
                task_id="summarize_publications",
                python_callable=self.summarize_publications,
                op_kwargs={
                    "num_searches": len(searches),
                },
            )

            # Task: Preparação de contexto para IA
            prepare_ai_context_task = PythonOperator(
                task_id="prepare_ai_context",
                python_callable=self.prepare_ai_context,
                op_kwargs={"num_searches": len(searches)},
            )

            # Task: Geração de resumo com IA
            generate_ai_summary_task = PythonOperator(
                task_id="generate_ai_summary",
                python_callable=self.generate_ai_summary,
                op_kwargs={"num_searches": len(searches)},
            )

            send_notification_task = PythonOperator(
                task_id="send_notification",
                python_callable=self.send_notification,
                op_kwargs={
                    "num_searches": len(searches),
                    "specs": specs,
                    "report_date": template_ano_mes_dia_trigger_local_time,
                },
            )

            # Fluxo com IA:
            tg_exec_searchs >> has_matches_task
            has_matches_task >> skip_notification_task
            has_matches_task >> summarize_publications_task >> prepare_ai_context_task >> generate_ai_summary_task >> send_notification_task

        return dag


# Run dag generation
DouDigestDagGenerator().generate_dags()